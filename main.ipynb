{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Internal ipython tool for setting figure size\n",
    "from IPython.core.pylabtools import figsize\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',index_col=0)\n",
    "test = pd.read_csv('test.csv',index_col=0)\n",
    "\n",
    "train_target = train['Sale_Price']\n",
    "test_target = test['Sale_Price']\n",
    "\n",
    "train = train.drop(columns=['Sale_Price'])\n",
    "test = test.drop(columns=['Sale_Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [col for col in train.columns if train[col].dtypes =='object']\n",
    "\n",
    "#dummy coding \n",
    "train = pd.get_dummies(train,columns = categorical_features)\n",
    "test = pd.get_dummies(test,columns = categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features,test_features = train.align(test,join = 'inner',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = list(train_features.columns)\n",
    "\n",
    "# Convert to np array\n",
    "features_tree = np.array(train_features)\n",
    "labels = np.array(train_target).reshape((-1, ))\n",
    "\n",
    "# Empty array for feature importances\n",
    "feature_importance_values = np.zeros(len(feature_names))\n",
    "\n",
    "for i in range(50):\n",
    "    model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n",
    "    # If training using early stopping need a validation set\n",
    "    model.fit(features_tree, labels)\n",
    "\n",
    "    # Record the feature importances\n",
    "    feature_importance_values += model.feature_importances_ / 50\n",
    "\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "\n",
    "# Sort features according to importance\n",
    "feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "\n",
    "# Normalize the feature importances to add up to one\n",
    "feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n",
    "feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n",
    "\n",
    "# Extract the features with zero importance\n",
    "record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.drop(columns=list(record_zero_importance['feature']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features,test_features = train_features.align(test_features,join = 'inner',axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Replace the inf and -inf with nan (required for later imputation)\n",
    "train_features = train_features.replace({np.inf: np.nan, -np.inf: np.nan})\n",
    "test_features = test_features.replace({np.inf: np.nan, -np.inf: np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute the data\n",
    "from sklearn.preprocessing import StandardScaler,Imputer\n",
    "\n",
    "im = Imputer(strategy = 'median')\n",
    "im.fit(train_features)\n",
    "train_features = im.fit_transform(train_features)\n",
    "test_features = im.fit_transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.where(~np.isfinite(train_features)))\n",
    "print(np.where(~np.isfinite(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_features)\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse \n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = np.array(train_target).reshape((-1, ))\n",
    "test_target = np.array(test_target).reshape((-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "gbm.fit(train_features,train_target)\n",
    "result = gbm.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "606224509.6952101"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(result,test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24621.62686938477\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(math.sqrt(mse(result,test_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter training \n",
    "# Loss function to be optimized\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "loss = ['ls', 'lad', 'huber']\n",
    "\n",
    "# Number of trees used in the boosting process\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "\n",
    "# Maximum depth of each tree\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "\n",
    "# Minimum number of samples per leaf\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "# Minimum number of samples to split a node\n",
    "min_samples_split = [2, 4, 6, 10]\n",
    "\n",
    "# Maximum number of features to consider for making splits\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {'loss': loss,\n",
    "                       'n_estimators': n_estimators,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model to use for hyperparameter tuning\n",
    "model = GradientBoostingRegressor(random_state = 42)\n",
    "\n",
    "# Set up the random search with 4-fold cross validation\n",
    "random_cv = RandomizedSearchCV(estimator=model,\n",
    "                               param_distributions=hyperparameter_grid,\n",
    "                               cv=4, n_iter=25, \n",
    "                               scoring = 'neg_mean_absolute_error',\n",
    "                               n_jobs = -1, verbose = 1, \n",
    "                               return_train_score = True,\n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4, error_score='raise',\n",
       "          estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=42,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=25, n_jobs=-1,\n",
       "          param_distributions={'loss': ['ls', 'lad', 'huber'], 'n_estimators': [100, 500, 900, 1100, 1500], 'max_depth': [2, 3, 5, 10, 15], 'min_samples_leaf': [1, 2, 4, 6, 8], 'min_samples_split': [2, 4, 6, 10], 'max_features': ['auto', 'sqrt', 'log2', None]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "          verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit on the training data\n",
    "random_cv.fit(train_features,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=6,\n",
       "             min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=1500, presort='auto', random_state=42,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of trees to evaluate\n",
    "trees_grid = {'n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800]}\n",
    "\n",
    "model = GradientBoostingRegressor(loss = 'huber', max_depth = 3,\n",
    "                                  min_samples_leaf = 2,\n",
    "                                  min_samples_split = 4,\n",
    "                                  max_features = 'auto',\n",
    "                                  random_state = 42)\n",
    "\n",
    "# Grid Search Object using the trees range and the random forest model\n",
    "grid_search = GridSearchCV(estimator = model, param_grid=trees_grid, cv = 4, \n",
    "                           scoring = 'neg_mean_absolute_error', verbose = 1,\n",
    "                           n_jobs = -1, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 15 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='huber', max_depth=3,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=2, min_samples_split=4,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_absolute_error', verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search\n",
    "grid_search.fit(train_features,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='huber', max_depth=3,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=2, min_samples_split=4,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=800,\n",
       "             presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the best model\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_model.fit(train_features,train_target)\n",
    "result = final_model.predict(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysubmission1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysubmission1[\"PID\"] = test['PID']\n",
    "mysubmission1['Sale_Price'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysubmission1.to_csv(\"mysubmission1.txt\",sep=\",\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='huber', max_depth=3,\n",
    "             max_features='auto', max_leaf_nodes=None,\n",
    "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "             min_samples_leaf=2, min_samples_split=4,\n",
    "             min_weight_fraction_leaf=0.0, n_estimators=700,\n",
    "             presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
    "             warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model.fit(train_features,train_target)\n",
    "result = second_model.predict(test_features)\n",
    "mysubmission2 = pd.DataFrame()\n",
    "mysubmission2[\"PID\"] = test['PID']\n",
    "mysubmission2['Sale_Price'] = result\n",
    "mysubmission2.to_csv(\"mysubmission2.txt\",sep=\",\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1345.7330601215363\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
